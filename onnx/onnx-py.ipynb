{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe7380b-e9b9-457a-bdf3-eabeae4b43d3",
   "metadata": {},
   "source": [
    "# Create Linear Model trivial ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39897ce6-4ab9-44b8-8262-aad96635d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import TensorProto\n",
    "from onnx.helper import (\n",
    "    make_model, make_node, make_graph,\n",
    "    make_tensor_value_info)\n",
    "from onnx.checker import check_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a5111e-9542-4a90-b33e-7809b092a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 5\n",
    "X = make_tensor_value_info('X', TensorProto.FLOAT, [\"batch_size\", feature_dim])\n",
    "Y = make_tensor_value_info('Y', TensorProto.FLOAT, [\"batch_size\"])\n",
    "A_initializer = onnx.helper.make_tensor('A', TensorProto.FLOAT, [feature_dim], [100.0, 10.0, 1.0, 0.1, 0.01])\n",
    "B_initializer = onnx.helper.make_tensor('B', TensorProto.FLOAT, [], [7000.0])\n",
    "node1 = make_node('MatMul', ['X', 'A'], ['XA'], 'XA')\n",
    "node2 = make_node('Add', ['XA', 'B'], ['Y'], 'Y')\n",
    "graph = make_graph([node1, node2], 'lr', [X], [Y], initializer=[A_initializer, B_initializer])\n",
    "onnx_model = make_model(graph)\n",
    "check_model(onnx_model)\n",
    "with open(\"linear_regression.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740e9376-6c3b-4c0f-b15c-2e7c10f80174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]]\n",
      "[7123.45 7679.  ]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(10, dtype=np.float32)+1\n",
    "x = np.reshape(x, (2, 5))\n",
    "print(x)\n",
    "ort_sess = ort.InferenceSession('linear_regression.onnx')\n",
    "outputs = ort_sess.run(['Y'], {'X': x})\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2477f0c-1ed3-4c39-aa1c-2a76dfb195c6",
   "metadata": {},
   "source": [
    "### MatMul WAT:  How does it work on the edge cases ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0933b3-8e72-4993-9029-35a329c20147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 7, 32)\n",
      "(12, 32, 7)\n",
      "(2, 12, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "lhs = (np.arange(2 * 1 * 7 * 32, dtype=np.float32)+1) / 1000.0\n",
    "lhs = np.reshape(lhs, (2, 1, 7, 32))\n",
    "print(lhs.shape)\n",
    "rhs = (np.arange(12*7*32, dtype=np.float32)+1) / 1000.0\n",
    "rhs = np.reshape(rhs, (12, 32, 7))\n",
    "print(rhs.shape)\n",
    "res = np.matmul(rhs, lhs)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a6f50-c45a-47cb-a297-78b92b525cf9",
   "metadata": {},
   "source": [
    "# Experimenting with model [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "Normalization formulation:\n",
    "\n",
    "$$\n",
    "v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n",
    "$$\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c9d79b-48fa-41c8-929f-9850b587a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0e2fe-16c9-41cc-a4ec-fb6aaf4e28f8",
   "metadata": {},
   "source": [
    "### Imports, create `tokenizer` and `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b980b1-3745-40f6-98da-f22fe864c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 07:39:20.623591: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-31 07:39:20.752880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 07:39:20.795165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 07:39:20.811348: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 07:39:20.914578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 07:39:21.558344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b6904-6073-4070-bcbd-5e44a9ef799c",
   "metadata": {},
   "source": [
    "### Sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7d2828-33ff-4e49-905e-d01214cfd1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input:\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "print(\"Encoded input:\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "611c645b-6724-44fa-859b-55d8db3b696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
       "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]),\n",
       " 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e158d375-c48f-432c-a9c5-4324ebb24409",
   "metadata": {},
   "source": [
    "### Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3aad9c06-6b29-43a9-b368-dd4af93a8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 384)\n",
      "[[[ 0.03656479 -0.01616146  0.1682453  ...  0.05540764 -0.16443957\n",
      "   -0.29669833]\n",
      "  [ 0.7239094   0.6399461   0.18878399 ...  0.5945502   0.6205655\n",
      "    0.489683  ]\n",
      "  [ 0.00637847  0.02030473  0.04475658 ...  0.34638238  1.3169885\n",
      "   -0.16695468]\n",
      "  ...\n",
      "  [ 0.1479177  -0.06426162  0.14569402 ...  0.8837387  -0.33155778\n",
      "    0.2975315 ]\n",
      "  [ 0.52124625  0.6562965   0.5607001  ... -0.03988977  0.04121367\n",
      "   -1.4035654 ]\n",
      "  [ 1.0824106   0.7140344   0.39859214 ... -0.23005268  0.32431406\n",
      "   -1.0312778 ]]\n",
      "\n",
      " [[ 0.2802185   0.11647302 -0.04178832 ...  0.27105364 -0.16846775\n",
      "   -0.29611403]\n",
      "  [ 0.87294626  0.4544794  -0.10909736 ...  0.13654931  0.45797268\n",
      "   -0.20415133]\n",
      "  [ 0.4751616   0.5731077   0.63044137 ...  0.6525696   0.5612419\n",
      "   -1.3268433 ]\n",
      "  ...\n",
      "  [ 0.61133045  0.79203445 -0.4684846  ...  0.08543227  1.0591549\n",
      "   -0.2983293 ]\n",
      "  [ 0.4115055   1.0945691   0.23854384 ...  0.8983636   0.3683571\n",
      "   -0.733289  ]\n",
      "  [ 0.13744976  0.55554354  0.26777348 ...  0.5426259   0.46651605\n",
      "   -0.52835524]]]\n"
     ]
    }
   ],
   "source": [
    "ort_sess = ort.InferenceSession('model.onnx')\n",
    "outputKey = 'last_hidden_state'\n",
    "inputs = {key: value.numpy() for key, value in encoded_input.data.items()}\n",
    "modelOutput = ort_sess.run([outputKey], inputs)[0]\n",
    "print(f\"{modelOutput.shape}\")\n",
    "print(modelOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5763b114-bb6d-4ea0-af92-e65ea136d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/embeddings/word_embeddings/Gather_output_0: f(7, 384)\n",
      "[[-0.0176194  -0.00760055  0.04710554 ... -0.05453258  0.0075766\n",
      "  -0.06167737]\n",
      " [-0.00191669 -0.00740279 -0.02672139 ... -0.00103431  0.01970551\n",
      "   0.18913172]\n",
      " [-0.02077937 -0.02789463 -0.0515293  ...  0.02073444  0.03772346\n",
      "   0.06095179]\n",
      " ...\n",
      " [-0.00860905  0.02101669 -0.00808366 ... -0.00936622  0.01282229\n",
      "   0.11787558]\n",
      " [-0.00389632 -0.04506262  0.06088516 ... -0.07729588  0.05493404\n",
      "  -0.05950066]\n",
      " [ 0.03316601 -0.0085353  -0.03998309 ...  0.02072961 -0.00336355\n",
      "  -0.00039823]]\n"
     ]
    }
   ],
   "source": [
    "def probeOnnxNodeOutput(node_output_names, inputs):\n",
    "    model = onnx.load(\"model.onnx\")\n",
    "    model.graph.output.extend([onnx.ValueInfoProto(name=node_name) for node_name in node_output_names])\n",
    "    onnx.save(model, \"modified_model.onnx\") \n",
    "    ort_sess = ort.InferenceSession('modified_model.onnx')\n",
    "    outputs = ort_sess.run(node_output_names, inputs)[0]\n",
    "    for (name, output) in zip(node_output_names, outputs):\n",
    "        print(f\"\\n{name}: f{output.shape}\")\n",
    "        print(output)\n",
    "\n",
    "probeOnnxNodeOutput([\"/embeddings/word_embeddings/Gather_output_0\"], inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ca9f7-cef6-41a6-983b-d7b01ab28db4",
   "metadata": {},
   "source": [
    "### Model Inference with HuggingFace/PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99cc66c8-139b-4b4f-9de5-49a38de3a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 384])\n",
      "tensor([[[ 0.0366, -0.0162,  0.1682,  ...,  0.0554, -0.1644, -0.2967],\n",
      "         [ 0.7239,  0.6399,  0.1888,  ...,  0.5946,  0.6206,  0.4897],\n",
      "         [ 0.0064,  0.0203,  0.0448,  ...,  0.3464,  1.3170, -0.1670],\n",
      "         ...,\n",
      "         [ 0.1479, -0.0643,  0.1457,  ...,  0.8837, -0.3316,  0.2975],\n",
      "         [ 0.5212,  0.6563,  0.5607,  ..., -0.0399,  0.0412, -1.4036],\n",
      "         [ 1.0824,  0.7140,  0.3986,  ..., -0.2301,  0.3243, -1.0313]],\n",
      "\n",
      "        [[ 0.2802,  0.1165, -0.0418,  ...,  0.2711, -0.1685, -0.2961],\n",
      "         [ 0.8729,  0.4545, -0.1091,  ...,  0.1365,  0.4580, -0.2042],\n",
      "         [ 0.4752,  0.5731,  0.6304,  ...,  0.6526,  0.5612, -1.3268],\n",
      "         ...,\n",
      "         [ 0.6113,  0.7920, -0.4685,  ...,  0.0854,  1.0592, -0.2983],\n",
      "         [ 0.4115,  1.0946,  0.2385,  ...,  0.8984,  0.3684, -0.7333],\n",
      "         [ 0.1374,  0.5555,  0.2678,  ...,  0.5426,  0.4665, -0.5284]]])\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "print(f\"{model_output.last_hidden_state.shape}\")\n",
    "print(model_output.last_hidden_state)\n",
    "\n",
    "if False:\n",
    "    # Disabled for now\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    print(f\"Sentence embeddings: {sentence_embeddings.shape}\")\n",
    "    print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2df8015-c059-4ca6-8646-2b9a4f679b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'last_hidden_state',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pooler_output',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d76a049-b78f-431c-95a9-32b020b0544c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'last_hidden_state': tensor([[[ 0.0366, -0.0162,  0.1682,  ...,  0.0554, -0.1644, -0.2967],\n",
       "                       [ 0.7239,  0.6399,  0.1888,  ...,  0.5946,  0.6206,  0.4897],\n",
       "                       [ 0.0064,  0.0203,  0.0448,  ...,  0.3464,  1.3170, -0.1670],\n",
       "                       ...,\n",
       "                       [ 0.1479, -0.0643,  0.1457,  ...,  0.8837, -0.3316,  0.2975],\n",
       "                       [ 0.5212,  0.6563,  0.5607,  ..., -0.0399,  0.0412, -1.4036],\n",
       "                       [ 1.0824,  0.7140,  0.3986,  ..., -0.2301,  0.3243, -1.0313]],\n",
       "              \n",
       "                      [[ 0.2802,  0.1165, -0.0418,  ...,  0.2711, -0.1685, -0.2961],\n",
       "                       [ 0.8729,  0.4545, -0.1091,  ...,  0.1365,  0.4580, -0.2042],\n",
       "                       [ 0.4752,  0.5731,  0.6304,  ...,  0.6526,  0.5612, -1.3268],\n",
       "                       ...,\n",
       "                       [ 0.6113,  0.7920, -0.4685,  ...,  0.0854,  1.0592, -0.2983],\n",
       "                       [ 0.4115,  1.0946,  0.2385,  ...,  0.8984,  0.3684, -0.7333],\n",
       "                       [ 0.1374,  0.5555,  0.2678,  ...,  0.5426,  0.4665, -0.5284]]]),\n",
       "              'pooler_output': tensor([[ 1.3429e-02,  4.0036e-02,  3.0797e-03,  7.7095e-03, -8.5741e-02,\n",
       "                       -3.2874e-02,  4.5395e-02,  5.4421e-02, -6.6219e-02, -3.3736e-02,\n",
       "                       -7.4499e-03,  3.3775e-02, -1.8523e-02, -1.2477e-02, -6.1699e-02,\n",
       "                        7.9306e-02,  9.3979e-02, -2.9625e-02, -1.4692e-02,  5.6033e-02,\n",
       "                        1.1484e-02,  1.1056e-02,  2.2872e-02, -2.9034e-02, -1.8242e-02,\n",
       "                        1.3069e-01, -2.4484e-02,  5.1790e-02,  3.6784e-02,  8.1075e-02,\n",
       "                        8.6604e-02,  3.3906e-04, -6.8685e-02,  3.2757e-02,  2.5934e-03,\n",
       "                       -4.3434e-02, -1.7191e-02,  8.2270e-02, -4.7278e-02, -3.3682e-02,\n",
       "                        6.5672e-02,  2.9311e-02, -5.9559e-02,  7.0777e-02, -1.5764e-02,\n",
       "                        2.1118e-02, -1.0806e-01, -3.4479e-02, -4.8619e-02,  4.2684e-02,\n",
       "                       -1.2006e-01,  4.6358e-02,  1.5641e-02, -9.6469e-03, -3.7291e-02,\n",
       "                        3.7824e-02,  3.4880e-02, -1.5224e-02,  5.4148e-02,  2.4613e-02,\n",
       "                        1.5847e-02, -2.4202e-02, -2.2712e-03, -7.8123e-03, -9.2186e-02,\n",
       "                        4.1878e-03, -5.9704e-02, -1.0853e-01, -3.1122e-02, -2.0779e-02,\n",
       "                       -3.8755e-02,  2.9059e-02, -4.6478e-02, -2.9518e-03,  2.0831e-02,\n",
       "                        5.2982e-02, -4.9873e-02,  2.5957e-02, -6.8848e-02, -9.3442e-04,\n",
       "                        9.7473e-03,  3.8063e-02,  6.7740e-02, -1.2013e-01, -4.9044e-02,\n",
       "                       -7.2494e-02, -5.4891e-02,  4.2310e-02, -7.8360e-03,  7.4343e-02,\n",
       "                        4.2601e-02,  1.1231e-02, -1.3430e-01,  5.6985e-02,  8.3218e-02,\n",
       "                       -2.5391e-02, -6.5197e-03, -5.7111e-02, -1.7963e-01, -4.5549e-03,\n",
       "                        7.8273e-02, -9.1693e-03, -2.6726e-02,  2.8545e-02, -2.9945e-03,\n",
       "                       -8.1081e-02, -1.1999e-02,  7.7119e-02, -2.2280e-02,  3.5329e-02,\n",
       "                        9.1938e-02,  2.5805e-02,  4.1517e-02,  1.5394e-02, -4.7167e-02,\n",
       "                        7.5881e-02, -1.8906e-02, -3.8819e-02, -1.1037e-01,  6.3771e-02,\n",
       "                        1.2736e-01, -4.3087e-02,  6.0566e-02,  4.5662e-02,  1.2671e-02,\n",
       "                        1.6946e-03,  4.5216e-02, -6.5698e-02, -9.2475e-02, -4.4247e-02,\n",
       "                        1.1738e-01, -4.1540e-02,  9.1604e-02, -1.2345e-01, -6.4776e-02,\n",
       "                        5.9695e-02, -4.8307e-02, -1.9186e-02, -4.8370e-02,  8.4520e-02,\n",
       "                        6.1380e-02, -1.3651e-01,  4.2905e-02, -3.0625e-02,  7.8972e-02,\n",
       "                        1.2373e-02, -7.8566e-02, -1.1315e-01,  2.2296e-03, -2.0005e-02,\n",
       "                        7.4801e-02, -7.9967e-02, -9.2751e-02,  4.6135e-02,  4.7488e-03,\n",
       "                        1.9638e-02,  3.5892e-02, -2.1168e-02, -1.0492e-02, -5.3379e-02,\n",
       "                       -4.8980e-02, -7.6467e-02,  8.6475e-02,  1.7718e-02, -5.9111e-02,\n",
       "                        2.3288e-02,  2.2532e-02, -3.4328e-02, -2.9752e-02, -6.2304e-02,\n",
       "                       -8.1938e-02,  1.7710e-02, -4.8280e-02, -6.2486e-02,  3.1141e-02,\n",
       "                        2.3507e-02,  5.2302e-02, -6.9293e-02,  1.1098e-02, -2.0867e-02,\n",
       "                        5.7609e-02, -1.0762e-01, -6.3090e-02, -3.2820e-02,  5.2514e-02,\n",
       "                        3.2342e-02,  3.6938e-02,  7.0675e-02,  1.0392e-01, -5.7706e-02,\n",
       "                        1.6733e-02,  1.4690e-02,  2.7245e-02,  5.0453e-02, -8.0111e-03,\n",
       "                       -3.9295e-02, -7.9695e-02,  1.6183e-02,  2.3606e-02,  1.4193e-02,\n",
       "                        1.6472e-02,  7.4927e-02, -4.1723e-03,  4.2714e-02, -1.2629e-02,\n",
       "                       -2.7271e-02, -6.2421e-02, -1.4004e-01, -2.9299e-02,  1.6869e-02,\n",
       "                        6.9726e-02,  2.7200e-02, -4.3012e-02, -3.9393e-02, -6.9088e-02,\n",
       "                       -5.2691e-02,  8.2750e-02,  7.5312e-02,  8.0191e-02, -1.2825e-02,\n",
       "                        4.1995e-02, -1.2516e-02,  2.7372e-02, -1.2028e-01, -1.2387e-01,\n",
       "                       -5.4954e-02, -6.5398e-03, -6.6795e-02,  1.3161e-02,  2.2269e-02,\n",
       "                       -2.9861e-02,  7.6297e-02,  2.7364e-02,  1.0883e-01, -8.8141e-03,\n",
       "                        8.6819e-02, -2.4719e-02, -4.9682e-02,  5.1936e-02,  3.4862e-02,\n",
       "                       -1.2001e-01,  8.3150e-02, -2.0191e-02, -5.3333e-03, -7.3319e-02,\n",
       "                        4.7377e-02,  6.8626e-02,  5.7092e-02,  1.0385e-02, -3.3254e-02,\n",
       "                        1.5860e-02,  1.5843e-02,  3.2043e-03,  3.3453e-02, -2.3585e-02,\n",
       "                        5.4750e-02,  2.8557e-02, -2.5185e-02,  2.1314e-02,  3.8470e-02,\n",
       "                        4.0113e-02,  1.6145e-02, -5.4575e-02, -1.4632e-03, -4.4623e-02,\n",
       "                        2.8726e-02,  1.1302e-01, -4.1902e-02, -1.3932e-02, -9.6624e-02,\n",
       "                        1.8770e-02,  9.0820e-03, -8.1532e-02, -8.8747e-03,  1.0511e-01,\n",
       "                        2.2679e-02,  2.5884e-02, -7.2527e-02,  6.1241e-02, -1.5532e-02,\n",
       "                        1.4980e-02,  5.6270e-02,  6.3009e-02, -4.2786e-02,  7.4515e-02,\n",
       "                       -2.7271e-02,  4.7316e-02, -2.8739e-02,  4.9152e-02,  8.2691e-02,\n",
       "                        1.2656e-02, -2.7052e-02, -4.3005e-02,  9.0674e-03, -5.3151e-02,\n",
       "                        6.2785e-02,  3.9300e-02,  6.7608e-02, -8.9390e-03,  3.2900e-02,\n",
       "                       -3.2308e-02, -3.5465e-02,  7.5596e-02, -2.9816e-02, -3.6290e-02,\n",
       "                        1.5477e-02, -4.8453e-02,  7.6023e-03,  3.5700e-02,  1.4346e-02,\n",
       "                        8.0476e-02,  6.1430e-02,  5.6743e-03,  1.1575e-02, -4.4946e-02,\n",
       "                       -3.3531e-02,  1.4398e-02, -1.3222e-01, -4.2706e-03, -3.7443e-02,\n",
       "                       -1.4443e-02, -7.8702e-02,  2.7131e-02,  6.0028e-02,  5.8510e-02,\n",
       "                       -1.2207e-02,  2.7320e-02,  3.1169e-02, -4.1544e-02, -2.1317e-02,\n",
       "                        1.6517e-02, -2.7584e-02,  3.4797e-02,  4.2613e-02,  1.3636e-03,\n",
       "                       -8.5100e-02,  3.7460e-02,  4.3458e-02, -9.4549e-03,  3.9982e-02,\n",
       "                        4.2519e-02, -8.8832e-02, -7.8077e-03, -1.1596e-01,  9.5078e-02,\n",
       "                       -9.7206e-02,  6.5254e-02,  2.9393e-02,  3.7239e-03, -2.1312e-02,\n",
       "                        1.9388e-02, -5.0150e-02,  6.9220e-02, -1.0495e-02, -2.4837e-02,\n",
       "                       -7.5547e-02, -2.7963e-02, -2.5091e-03,  6.8878e-02, -4.4469e-02,\n",
       "                       -3.9632e-02, -1.1323e-02, -5.2578e-02, -8.7117e-02,  2.7642e-02,\n",
       "                       -4.9512e-02,  2.0677e-02, -1.1490e-02,  1.2818e-02,  2.9686e-02,\n",
       "                       -1.0459e-01,  5.0408e-02,  1.0962e-01,  4.6272e-02, -1.1506e-04,\n",
       "                        6.5767e-03, -8.2945e-02,  5.0269e-02, -5.0819e-02, -6.9119e-02,\n",
       "                        6.4439e-02,  3.8906e-02, -6.8270e-02, -9.6023e-03],\n",
       "                      [-1.6412e-02,  2.0522e-02,  2.4394e-02,  7.2660e-02, -1.1512e-01,\n",
       "                        2.3330e-02,  4.0078e-03,  5.9773e-02, -5.6792e-02,  1.5403e-02,\n",
       "                       -1.9215e-04,  1.2416e-02,  4.9173e-02, -4.5729e-03, -1.0675e-01,\n",
       "                        6.8074e-02,  1.2190e-01, -8.6177e-03, -2.9459e-02,  9.0053e-02,\n",
       "                       -6.7227e-03,  1.7838e-02,  1.7992e-02, -6.7095e-02, -2.6161e-02,\n",
       "                        1.1410e-01,  1.9777e-03, -7.3730e-03,  4.2695e-02,  6.5730e-02,\n",
       "                        9.1121e-02,  2.3741e-02, -8.6613e-02,  3.9649e-02, -2.5506e-02,\n",
       "                       -2.1669e-02, -6.7568e-02,  1.0122e-01, -2.1715e-02,  2.3547e-03,\n",
       "                        8.2525e-02,  4.1297e-02, -7.3701e-02,  7.1083e-02, -1.2322e-02,\n",
       "                       -4.2826e-02, -1.2831e-01,  1.0527e-02,  5.1973e-04,  1.2830e-02,\n",
       "                       -1.3040e-01,  8.9542e-02,  8.1203e-02, -6.0886e-03, -6.3051e-02,\n",
       "                        5.1941e-02,  5.5326e-02,  1.4283e-02,  4.2129e-02,  2.1932e-02,\n",
       "                       -1.1775e-02, -3.3309e-02, -3.3257e-02, -5.0021e-02, -9.2995e-02,\n",
       "                        8.0433e-03, -6.2652e-02, -9.0668e-02, -2.9693e-02,  1.3278e-02,\n",
       "                       -1.2384e-02,  2.4324e-02, -1.6497e-02, -1.5386e-02,  7.7987e-03,\n",
       "                        8.2918e-02, -3.9625e-02,  3.0166e-03, -7.1665e-02,  3.2242e-02,\n",
       "                       -3.1861e-02,  5.1786e-02,  7.8073e-02, -1.3790e-01, -5.6443e-02,\n",
       "                       -8.6384e-02, -1.8559e-02,  9.2650e-02, -8.3894e-03,  6.5256e-02,\n",
       "                        4.7546e-03, -4.9196e-02, -1.5398e-01,  3.6895e-02,  1.2527e-01,\n",
       "                       -5.4036e-02, -9.4263e-03, -2.3183e-02, -2.2138e-01,  2.2980e-03,\n",
       "                        8.8022e-02, -6.1781e-02, -6.3716e-02,  3.4578e-02,  2.7633e-02,\n",
       "                       -6.0287e-02, -4.5254e-03,  4.8531e-02,  2.7681e-02, -1.8568e-02,\n",
       "                        1.1042e-01, -3.9831e-03,  3.4688e-02,  4.7291e-02, -4.1807e-02,\n",
       "                        3.1925e-02, -3.4659e-02, -7.9173e-02, -1.3209e-01,  1.0513e-01,\n",
       "                        1.0988e-01, -5.0374e-02,  9.4562e-03,  5.7217e-02,  2.4318e-02,\n",
       "                       -2.5906e-02,  7.1816e-02, -4.5563e-02, -7.4738e-02, -7.7397e-02,\n",
       "                        1.6668e-01, -2.5206e-02,  7.5145e-02, -8.2775e-02, -8.3848e-02,\n",
       "                        6.7262e-02, -2.4362e-02,  5.4103e-03, -1.7812e-02,  7.4477e-02,\n",
       "                        5.5941e-02, -1.7995e-01,  4.4545e-02, -5.4162e-02,  4.8428e-02,\n",
       "                       -3.0975e-03, -8.8899e-02, -7.1491e-02, -8.5257e-03, -6.4380e-02,\n",
       "                        1.2850e-01, -6.2238e-02, -8.5901e-02,  4.5637e-02, -2.5258e-02,\n",
       "                        2.0246e-02,  6.9027e-02, -2.5334e-02,  2.0954e-02, -9.4911e-02,\n",
       "                       -7.4533e-02, -9.7445e-02,  5.3053e-02,  6.4804e-02, -1.7467e-02,\n",
       "                        7.8287e-03,  5.3832e-02,  1.7221e-02, -4.1689e-02, -3.5192e-02,\n",
       "                       -5.9480e-02, -2.4315e-03, -5.2786e-02, -7.3131e-02,  3.8923e-02,\n",
       "                        2.5390e-02,  5.8073e-02, -1.1277e-01, -1.3115e-02,  3.7546e-03,\n",
       "                        5.8880e-02, -8.6683e-02, -1.1336e-01, -5.9326e-02,  1.5321e-02,\n",
       "                        3.7837e-02,  2.3076e-02,  3.6856e-02,  1.1715e-01, -7.1100e-02,\n",
       "                        5.0757e-03, -6.6637e-04,  6.2937e-02,  6.0390e-02, -3.4757e-02,\n",
       "                       -3.7994e-02, -1.0356e-01,  4.6443e-03, -5.8602e-04, -3.8506e-02,\n",
       "                       -1.8699e-03,  6.6083e-02, -4.0170e-02,  3.9136e-02,  5.1422e-02,\n",
       "                       -1.2792e-02, -1.2741e-01, -1.1371e-01, -7.4030e-03,  5.0568e-02,\n",
       "                        8.8432e-02,  4.5259e-02, -2.2429e-02, -4.3520e-02, -9.5628e-02,\n",
       "                       -5.2261e-02,  1.1536e-01,  8.3722e-02,  1.0216e-01, -1.2807e-02,\n",
       "                        2.7735e-02, -1.3866e-02,  2.4130e-03, -7.8098e-02, -8.1450e-02,\n",
       "                       -4.9948e-02, -2.2054e-02, -7.6649e-02,  2.2537e-02,  1.5444e-02,\n",
       "                       -2.4304e-02,  1.0686e-01,  5.5686e-02,  1.2883e-01, -2.3435e-02,\n",
       "                        1.1903e-01, -7.6298e-03, -3.7377e-02,  9.4343e-02,  3.0764e-02,\n",
       "                       -1.2493e-01,  1.4389e-01, -5.6773e-02, -1.0457e-02, -6.1964e-02,\n",
       "                        7.7382e-02,  1.1770e-01,  3.2610e-02,  3.9971e-02, -3.2292e-02,\n",
       "                        3.0921e-02, -2.9455e-03, -3.9535e-02,  1.6244e-02, -1.2068e-02,\n",
       "                        7.4872e-02,  2.4877e-02, -6.3048e-02,  8.2138e-02,  5.6135e-02,\n",
       "                        2.2832e-02, -1.5446e-02, -5.2837e-02,  1.2114e-02,  1.3555e-02,\n",
       "                        1.9292e-02,  9.3866e-02, -4.8043e-02, -4.0721e-02, -1.0090e-01,\n",
       "                        6.4862e-02,  1.0930e-02, -1.3199e-01, -3.3183e-02,  7.5214e-02,\n",
       "                        3.0600e-02,  9.9645e-03, -9.6374e-02,  7.0916e-02, -1.1066e-02,\n",
       "                        2.9517e-02,  4.1136e-02,  3.9592e-02, -6.6500e-02,  4.8720e-02,\n",
       "                       -7.3051e-03,  4.9139e-02, -2.4039e-02,  8.4684e-02,  1.5190e-01,\n",
       "                       -5.5679e-03, -1.6845e-02, -1.8212e-02,  4.7395e-02, -1.0036e-01,\n",
       "                        8.4382e-02,  4.0205e-02,  8.6075e-02, -2.0358e-02,  1.8569e-02,\n",
       "                       -6.9477e-02, -4.6497e-02,  9.4527e-02, -5.3387e-02, -4.0131e-02,\n",
       "                       -2.2027e-02, -8.5623e-02,  1.8590e-02,  3.0398e-02,  2.2257e-03,\n",
       "                        6.8550e-02,  6.2411e-02,  5.4371e-03,  7.3998e-02,  1.7568e-02,\n",
       "                        6.2321e-03,  6.0236e-02, -1.4625e-01,  4.7108e-03, -3.3739e-02,\n",
       "                       -3.1894e-03, -4.1378e-02,  3.4950e-02,  5.7236e-02,  4.0846e-02,\n",
       "                       -2.7780e-02,  5.6281e-02,  5.4090e-02,  1.2975e-02, -1.8945e-03,\n",
       "                        3.3885e-02, -8.3884e-02,  7.6936e-02,  5.1811e-02,  4.6538e-02,\n",
       "                       -1.0081e-01,  1.3060e-01,  2.5463e-02,  4.1129e-02,  7.5722e-02,\n",
       "                        3.9883e-02, -8.1436e-02, -2.9125e-02, -9.2279e-02,  1.0999e-01,\n",
       "                       -8.7998e-02,  1.7189e-02,  2.2057e-02,  5.7704e-04, -1.8650e-03,\n",
       "                        2.7676e-02,  1.9341e-03,  6.4815e-02,  1.2107e-02, -4.4955e-02,\n",
       "                       -1.0582e-01, -4.7512e-02,  3.1620e-02,  2.9339e-02,  2.8688e-02,\n",
       "                       -1.5993e-02, -2.3867e-02, -6.3935e-02, -9.3625e-02,  5.4746e-02,\n",
       "                       -4.6345e-02,  1.2985e-03, -2.4204e-02,  3.1631e-02,  3.0533e-02,\n",
       "                       -9.0704e-02,  7.9624e-02,  9.8593e-02,  3.1205e-02, -1.6831e-02,\n",
       "                        7.8870e-03, -1.1406e-01,  1.2890e-01, -5.1266e-02, -9.4784e-02,\n",
       "                        3.2232e-02,  3.0980e-02, -9.9598e-02, -6.3203e-03]])})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = model_output.values()\n",
    "v.mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
