{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f5cd05-8e1b-4873-8220-7cd7577da125",
   "metadata": {},
   "source": [
    "# Converting ONNX models to GoMLX\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdd74c0-e5af-45d6-93d8-7453b1a590f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**GoNB** version [v0.10.6](https://github.com/janpfeifer/gonb/releases/tag/v0.10.6) / Commit: [0e5f587a077810d058202b76a127651a02bd4382](https://github.com/janpfeifer/gonb/tree/0e5f587a077810d058202b76a127651a02bd4382)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Added replace rule for module \"github.com/gomlx/gomlx\" to local directory \"/home/janpf/Projects/gomlx\".\n",
      "\t- Added replace rule for module \"github.com/janpfeifer/gonb\" to local directory \"/home/janpf/Projects/gonb\".\n",
      "\t- Added replace rule for module \"github.com/gomlx/onnx-gomlx\" to local directory \"/home/janpf/Projects/onnx-gomlx\".\n",
      "\t- Added replace rule for module \"github.com/gomlx/go-huggingface\" to local directory \"/home/janpf/Projects/go-huggingface\".\n"
     ]
    }
   ],
   "source": [
    "%version\n",
    "!*rm -f go.work && go work init\n",
    "!*go work use . \"${HOME}/Projects/gonb\" \"${HOME}/Projects/gomlx\" \"${HOME}/Projects/onnx-gomlx\" \"${HOME}/Projects/go-huggingface\"\n",
    "%goworkfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adeca919-f2ed-4d8b-93fb-6978f94de2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "\t\"fmt\"\n",
    "\t\"os\"\n",
    "\n",
    "\t\"github.com/gogo/protobuf/proto\"\n",
    "\t\"github.com/janpfeifer/must\"\n",
    "\n",
    "\t\"github.com/gomlx/gomlx/backends\"\n",
    "    . \"github.com/gomlx/gomlx/graph\"\n",
    "\t\"github.com/gomlx/gomlx/ml/context\"\n",
    "\t\"github.com/gomlx/gomlx/ml/data\"\n",
    "\t\"github.com/gomlx/gomlx/types\"\n",
    "\t\"github.com/gomlx/gopjrt/dtypes\"\n",
    "    \"github.com/gomlx/onnx-gomlx/onnx\"\n",
    "    \"github.com/gomlx/go-huggingface/hub\"\n",
    "    \"github.com/gomlx/go-huggingface/tokenizers\"\n",
    "\t\"github.com/pkg/errors\"\n",
    "\n",
    "\t_ \"github.com/gomlx/gomlx/backends/xla\"\n",
    ")\n",
    "\n",
    "var (\n",
    "    _ = Add\n",
    "    backend = backends.New()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273d236-cb0d-4d92-a0f0-e9b9d6f6e156",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "\n",
    "The `linear_regression.onnx` was created manually using python (see accompanying `onnx-py.ipynb` notebook):\n",
    "\n",
    "```python\n",
    "feature_dim = 5\n",
    "X = make_tensor_value_info('X', TensorProto.FLOAT, [\"batch_size\", feature_dim])\n",
    "Y = make_tensor_value_info('Y', TensorProto.FLOAT, [\"batch_size\"])\n",
    "A_initializer = onnx.helper.make_tensor('A', TensorProto.FLOAT, [feature_dim], [100.0, 10.0, 1.0, 0.1, 0.01])\n",
    "B_initializer = onnx.helper.make_tensor('B', TensorProto.FLOAT, [], [7000.0])\n",
    "node1 = make_node('MatMul', ['X', 'A'], ['XA'], 'XA')\n",
    "node2 = make_node('Add', ['XA', 'B'], ['Y'], 'Y')\n",
    "graph = make_graph([node1, node2], 'lr', [X], [Y], initializer=[A_initializer, B_initializer])\n",
    "onnx_model = make_model(graph)\n",
    "check_model(onnx_model)\n",
    "with open(\"linear_regression.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c2fa3d-78e6-4283-be3d-1ccaa0ddffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model:\n",
      "\t# inputs:\t1\n",
      "\t\t[#0] X: (Float32) [batch_size, 5]\n",
      "\t# outputs:\t1\n",
      "\t\t[#0] Y: (Float32) [batch_size]\n",
      "\t# nodes:\t2\n",
      "\t# tensors (variables):\t2\n",
      "\t# sparse tensors (variables):\t0\n",
      "\tOp types:\t[]string{\"Add\", \"MatMul\"}\n",
      "\tIR Version:\t10\n",
      "\tOperator Sets:\t[v21]\n",
      "\n",
      "2 tensors (variables):\n",
      "\t\"A\": (Float32)[5]\n",
      "\t\"B\": (Float32)\n",
      "0 sparse tensors (variables)\n",
      "\n",
      "Model Graph \"lr\":\n",
      "\"XA\":\t[MatMul]\n",
      "\tInputs: [\"X\" \"A\"]\n",
      "\tOutputs: [\"XA\"]\n",
      "\"Y\":\t[Add]\n",
      "\tInputs: [\"XA\" \"B\"]\n",
      "\tOutputs: [\"Y\"]\n",
      "\n",
      "Variables loaded from \"/home/janpf/work/onnx/linear_regression.onnx\":\n",
      "\t- /ONNX/A: (Float32)[5]: []float32{100, 10, 1, 0.1, 0.01}\n",
      "\t- /ONNX/B: float32(7000)\n",
      "\n",
      "Example invocation:\n",
      "\tX*A+B=(Float32)[2]: []float32{7123.45, 7679}\n"
     ]
    }
   ],
   "source": [
    "var modelPath = data.ReplaceTildeInDir(\"~/work/onnx/linear_regression.onnx\") // all-MiniLM-L6-v2\n",
    "\n",
    "%%\n",
    "// Read and print the onnx model:\n",
    "model := must.M1(onnx.ReadFile(modelPath))\n",
    "fmt.Printf(\"%s\\n\", model)\n",
    "must.M(model.PrintVariables(os.Stdout))\n",
    "fmt.Println()\n",
    "must.M(model.PrintGraph(os.Stdout))\n",
    "fmt.Println()\n",
    "\n",
    "// Convert ONNX variables to GoMLX context (which stores variables):\n",
    "ctx := context.New()\n",
    "must.M(model.VariablesToContext(ctx))\n",
    "fmt.Printf(\"Variables loaded from %q:\\n\", modelPath)\n",
    "for v := range ctx.IterVariables() {\n",
    "    fmt.Printf(\"\\t- %s: %s\\n\", v.ScopeAndName(), v.Value().GoStr())\n",
    "}\n",
    "fmt.Println()\n",
    "\n",
    "// Execute it with GoMLX/XLA:\n",
    "gomlxFn := func(ctx *context.Context, x *Node) *Node {\n",
    "    \n",
    "    return model.CallGraph(ctx, x.Graph(), map[string]*Node{\"X\": x})[0]\n",
    "}\n",
    "x := [][]float32{{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}\n",
    "fmt.Println(\"Example invocation:\")\n",
    "fmt.Printf(\"\\tX*A+B=%v\\n\", context.ExecOnce(backend, ctx, gomlxFn, x).GoStr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80f42e-6886-43df-b592-77e4642ac356",
   "metadata": {},
   "source": [
    "## Updating a Model\n",
    "\n",
    "One of the main uses of `onnx-gomlx` is to convert the model to GoMLX and then fine-tune it. \n",
    "The fine-tuned (or simply updated) model can be used as usual as a GoMLX model, or the variables\n",
    "can be written back to the original ONNX model -- for intance if inference is being done somewhere else.\n",
    "\n",
    "Example:\n",
    "\n",
    "1. We load the Linear ONNX model\n",
    "2. We update the bias variable \"B\": from 7000 to 8000.\n",
    "3. We save the changed ONNX model.\n",
    "4. We re-run the example above, using the changed model, and observe B is now 8000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe1002e7-65ee-48eb-b888-41856ac6ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B value was float32(7000.0000)\n",
      "B value now is float32(8000.0000)\n",
      "Example invocation:\n",
      "\tX*A+B=[2]float32{8123.4502, 8679.0000}\n"
     ]
    }
   ],
   "source": [
    "var modelPath = data.ReplaceTildeInDir(\"~/work/onnx/linear_regression.onnx\") // all-MiniLM-L6-v2\n",
    "\n",
    "%%\n",
    "// 1. Load linear model:\n",
    "model := must.M1(onnx.ReadFile(modelPath))\n",
    "\n",
    "// 2. Convert ONNX variables to GoMLX context (which stores variables) and update \"B\" to 8000\n",
    "ctx := context.New()\n",
    "must.M(model.VariablesToContext(ctx))\n",
    "bVar := ctx.In(onnx.ModelScope).GetVariable(\"B\")\n",
    "fmt.Printf(\"B value was %s\\n\", bVar.Value())\n",
    "bVar.SetValue(tensors.FromValue(float32(8000)))\n",
    "\n",
    "// 3. Update the ONNX model variables, and save it.\n",
    "must.M(model.ContextToONNX(ctx))\n",
    "must.M(model.SaveToFile(modelPath+\"~\"))\n",
    "\n",
    "// 4. Re-run example on updated model:\n",
    "model = must.M1(onnx.ReadFile(modelPath+\"~\"))\n",
    "ctx = context.New()  // Create a new context.\n",
    "must.M(model.VariablesToContext(ctx))\n",
    "bVar = ctx.In(onnx.ModelScope).GetVariable(\"B\")\n",
    "fmt.Printf(\"B value now is %s\\n\", bVar.Value())\n",
    "gomlxFn := func(ctx *context.Context, x *Node) *Node {    \n",
    "    return model.CallGraph(ctx, x.Graph(), map[string]*Node{\"X\": x})[0]\n",
    "}\n",
    "x := [][]float32{{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}\n",
    "fmt.Println(\"Example invocation:\")\n",
    "fmt.Printf(\"\\tX*A+B=%v\\n\", context.ExecOnce(backend, ctx, gomlxFn, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ebe6e-4385-4254-aa01-1f3044a38fae",
   "metadata": {},
   "source": [
    "## Simple LSTM model\n",
    "\n",
    "The model was created with the Python code:\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0a0742-b851-4280-a38e-2f228818bea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model:\n",
      "\tProducer:\tpytorch / 2.5.0\n",
      "\t# inputs:\t1\n",
      "\t\t[#0] \"input\": (Int32) [1, sequence_length]\n",
      "\t# outputs:\t1\n",
      "\t\t[#0] \"output\": (Float32) [1, 3]\n",
      "\t# nodes:\t15\n",
      "\t# tensors (variables):\t6\n",
      "\t# sparse tensors (variables):\t0\n",
      "\tOp types:\t[]string{\"Concat\", \"Constant\", \"ConstantOfShape\", \"Gather\", \"Gemm\", \"LSTM\", \"Shape\", \"Squeeze\", \"Transpose\", \"Unsqueeze\"}\n",
      "\tIR Version:\t9\n",
      "\tOperator Sets:\t[v20]\n",
      "\n",
      "6 tensors (variables):\n",
      "\t\"embedding.weight\": (Float32)[30522 5]\n",
      "\t\"fc.weight\": (Float32)[3 11]\n",
      "\t\"fc.bias\": (Float32)[3]\n",
      "\t\"onnx::LSTM_108\": (Float32)[1 44 5]\n",
      "\t\"onnx::LSTM_109\": (Float32)[1 44 11]\n",
      "\t\"onnx::LSTM_110\": (Float32)[1 88]\n",
      "0 sparse tensors (variables)\n",
      "\n",
      "Model Graph \"main_graph\":\n",
      "\"/embedding/Gather\":\t[Gather]\n",
      "\tInputs: [\"embedding.weight\" \"input\"]\n",
      "\tOutputs: [\"/embedding/Gather_output_0\"]\n",
      "\"/lstm/Shape\":\t[Shape]\n",
      "\tInputs: [\"/embedding/Gather_output_0\"]\n",
      "\tOutputs: [\"/lstm/Shape_output_0\"]\n",
      "\"/lstm/Constant\":\t[Constant]\n",
      "\tInputs: []\n",
      "\tOutputs: [\"/lstm/Constant_output_0\"]\n",
      "\tAttributes: value (TENSOR: (Int64))\n",
      "\"/lstm/Gather\":\t[Gather]\n",
      "\tInputs: [\"/lstm/Shape_output_0\" \"/lstm/Constant_output_0\"]\n",
      "\tOutputs: [\"/lstm/Gather_output_0\"]\n",
      "\tAttributes: axis (INT: 0)\n",
      "\"/lstm/Constant_1\":\t[Constant]\n",
      "\tInputs: []\n",
      "\tOutputs: [\"/lstm/Constant_1_output_0\"]\n",
      "\tAttributes: value (TENSOR: (Int64)[1])\n",
      "\"Constant_10\":\t[Constant]\n",
      "\tInputs: []\n",
      "\tOutputs: [\"onnx::Unsqueeze_16\"]\n",
      "\tAttributes: value (TENSOR: (Int64)[1])\n",
      "\"/lstm/Unsqueeze\":\t[Unsqueeze]\n",
      "\tInputs: [\"/lstm/Gather_output_0\" \"onnx::Unsqueeze_16\"]\n",
      "\tOutputs: [\"/lstm/Unsqueeze_output_0\"]\n",
      "\"/lstm/Constant_2\":\t[Constant]\n",
      "\tInputs: []\n",
      "\tOutputs: [\"/lstm/Constant_2_output_0\"]\n",
      "\tAttributes: value (TENSOR: (Int64)[1])\n",
      "\"/lstm/Concat\":\t[Concat]\n",
      "\tInputs: [\"/lstm/Constant_1_output_0\" \"/lstm/Unsqueeze_output_0\" \"/lstm/Constant_2_output_0\"]\n",
      "\tOutputs: [\"/lstm/Concat_output_0\"]\n",
      "\tAttributes: axis (INT: 0)\n",
      "\"/lstm/ConstantOfShape\":\t[ConstantOfShape]\n",
      "\tInputs: [\"/lstm/Concat_output_0\"]\n",
      "\tOutputs: [\"/lstm/ConstantOfShape_output_0\"]\n",
      "\tAttributes: value (TENSOR: (Float32)[1])\n",
      "\"/lstm/Transpose\":\t[Transpose]\n",
      "\tInputs: [\"/embedding/Gather_output_0\"]\n",
      "\tOutputs: [\"/lstm/Transpose_output_0\"]\n",
      "\tAttributes: perm (INTS: [1 0 2])\n",
      "\"/lstm/LSTM\":\t[LSTM]\n",
      "\tInputs: [\"/lstm/Transpose_output_0\" \"onnx::LSTM_108\" \"onnx::LSTM_109\" \"onnx::LSTM_110\" \"\" \"/lstm/ConstantOfShape_output_0\" \"/lstm/ConstantOfShape_output_0\"]\n",
      "\tOutputs: [\"/lstm/LSTM_output_0\" \"/lstm/LSTM_output_1\" \"/lstm/LSTM_output_2\"]\n",
      "\tAttributes: hidden_size (INT: 11)\n",
      "\"/Constant\":\t[Constant]\n",
      "\tInputs: []\n",
      "\tOutputs: [\"/Constant_output_0\"]\n",
      "\tAttributes: value (TENSOR: (Int64)[1])\n",
      "\"/Squeeze\":\t[Squeeze]\n",
      "\tInputs: [\"/lstm/LSTM_output_1\" \"/Constant_output_0\"]\n",
      "\tOutputs: [\"/Squeeze_output_0\"]\n",
      "\"/fc/Gemm\":\t[Gemm]\n",
      "\tInputs: [\"/Squeeze_output_0\" \"fc.weight\" \"fc.bias\"]\n",
      "\tOutputs: [\"output\"]\n",
      "\tAttributes: alpha (FLOAT: 1.000000), beta (FLOAT: 1.000000), transB (INT: 1)\n",
      "\n",
      "Variables loaded from \"/home/janpf/work/onnx/test_lstm.onnx\":\n",
      "\t- /ONNX/embedding.weight: (Float32)[30522 5]\n",
      "\t- /ONNX/fc.weight: (Float32)[3 11]\n",
      "\t- /ONNX/fc.bias: (Float32)[3]\n",
      "\t- /ONNX/onnx::LSTM_108: (Float32)[1 44 5]\n",
      "\t- /ONNX/onnx::LSTM_109: (Float32)[1 44 11]\n",
      "\t- /ONNX/onnx::LSTM_110: (Float32)[1 88]\n",
      "\n",
      "lstm(x) = \t(Float32)[1 3]: [][]float32{{0.11684046, 0.15874876, 0.19921872}}\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "// Read and print the onnx model:\n",
    "modelPath := os.Getenv(\"HOME\")+\"/work/onnx/test_lstm.onnx\"\n",
    "model := must.M1(onnx.ReadFile(modelPath))\n",
    "fmt.Printf(\"%s\\n\", model)\n",
    "must.M(model.PrintVariables(os.Stdout))\n",
    "fmt.Println()\n",
    "must.M(model.PrintGraph(os.Stdout))\n",
    "fmt.Println()\n",
    "\n",
    "// Convert ONNX variables to GoMLX context (which stores variables):\n",
    "ctx := context.New()\n",
    "must.M(model.VariablesToContext(ctx))\n",
    "fmt.Printf(\"Variables loaded from %q:\\n\", modelPath)\n",
    "for v := range ctx.IterVariables() {\n",
    "    fmt.Printf(\"\\t- %s: %s\\n\", v.ScopeAndName(), v.Shape())\n",
    "}\n",
    "fmt.Println()\n",
    "\n",
    "// Execute it with GoMLX/XLA:\n",
    "gomlxFn := func(ctx *context.Context, x *Node) *Node {\n",
    "    return model.CallGraph(ctx, x.Graph(), map[string]*Node{\"input\": x})[0]\n",
    "}\n",
    "x := [][]int32{{0, 1, 2, 3, 4, 5, 6}}\n",
    "fmt.Printf(\"lstm(x) = \\t%v\\n\", context.ExecOnce(backend, ctx, gomlxFn, x).GoStr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d407a9d0-75df-465f-bf17-e341598f33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph.dot\n",
      "graph.svg\n",
      "graph.txt\n",
      "linear_regression.onnx\n",
      "model.onnx\n",
      "model_shapes_fixed.onnx\n",
      "model_shapes.txt\n",
      "modified_model.onnx\n",
      "onnx-go.ipynb\n",
      "onnx-py.ipynb\n",
      "test_lstm.onnx\n",
      "test_.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19434069-043d-4819-9cc7-1506ad31a143",
   "metadata": {},
   "source": [
    "## [Sentence Enconding all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "From the downloaded file `model.onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2240c215-8169-4e17-a64e-2628c2bbdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "var (\n",
    "    // Model IDs to use.\n",
    "    miniID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    bertID = \"KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    protectID = \"protectai/deberta-v3-base-zeroshot-v1-onnx\"\n",
    "    robertaID = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "    distillbertID = \"onnxport/distilbert-base-uncased-onnx\"\n",
    ")\n",
    "\n",
    "func onnxTest(modelID string, verbose bool, targetOutputs ...string) []*tensors.Tensor {\n",
    "    repo := hub.New(modelID)\n",
    "    modelFileName := \"model.onnx\"\n",
    "    if !repo.HasFile(modelFileName) { \n",
    "        modelFileName = \"onnx/model.onnx\"\n",
    "        if !repo.HasFile(modelFileName) { \n",
    "            log.Printf(\"Could not find \\\"model.onnx\\\" for repo %q\", repo)\n",
    "        }\n",
    "    }\n",
    "    modelPath := must.M1(repo.DownloadFile(modelFileName))\n",
    "    model := must.M1(onnx.ReadFile(modelPath))\n",
    "    if verbose {\n",
    "        fmt.Printf(\"\\n%s\\n\", modelID)\n",
    "        fmt.Printf(\"%s\\n\", model)\n",
    "    }\n",
    "        \n",
    "    // Create a file for debugging with detailed graph.\n",
    "    if verbose {\n",
    "        debugFile := must.M1(os.Create(\"graph.txt\"))\n",
    "        must.M(model.PrintVariables(debugFile))\n",
    "        fmt.Println()\n",
    "        must.M(model.PrintGraph(debugFile))\n",
    "        fmt.Println()\n",
    "        must.M(debugFile.Close())\n",
    "\n",
    "        dotFile := must.M1(os.Create(\"graph.dot\"))\n",
    "        must.M(model.PrintGraphviz(dotFile))\n",
    "        must.M(dotFile.Close())\n",
    "    }\n",
    "    \n",
    "    // Convert ONNX variables to GoMLX context (which stores variables):\n",
    "    ctx := context.New()\n",
    "    must.M(model.VariablesToContext(ctx))\n",
    "    \n",
    "    // Execute it with GoMLX/XLA:\n",
    "    sentences := []string{\n",
    "        \"This is an example sentence\", \n",
    "        \"Each sentence is converted\"}\n",
    "\n",
    "    // Encoding to tokens done in Python and pasted here.\n",
    "    inputIDs := [][]int64{\n",
    "        {101, 2023, 2003, 2019, 2742, 6251,  102},\n",
    "        { 101, 2169, 6251, 2003, 4991,  102,    0}}\n",
    "    tokenTypeIDs := [][]int64{\n",
    "        {0, 0, 0, 0, 0, 0, 0},\n",
    "        {0, 0, 0, 0, 0, 0, 0}}\n",
    "    attentionMask := [][]int64{\n",
    "        {1, 1, 1, 1, 1, 1, 1},\n",
    "        {1, 1, 1, 1, 1, 1, 0}}\n",
    "    if verbose {\n",
    "        fmt.Println(\"Example invocation:\")\n",
    "        fmt.Printf(\"\\tsentences: %q\\n\", sentences)\n",
    "    }\n",
    "    var embeddings []*tensors.Tensor\n",
    "    err := exceptions.TryCatch[error](func() {\n",
    "        embeddings = context.ExecOnceN(\n",
    "            backend, ctx, func (ctx *context.Context, inputs []*Node) []*Node {\n",
    "                inputsMap := map[string]*Node{\n",
    "                    \"input_ids\": inputs[0],\n",
    "                    \"attention_mask\": inputs[1]}\n",
    "                if model.NumInputs() == 3 {\n",
    "                    inputsMap[\"token_type_ids\"] = inputs[2]\n",
    "                }\n",
    "                return model.CallGraph(ctx, inputs[0].Graph(), inputsMap, targetOutputs...)\n",
    "            }, inputIDs, attentionMask, tokenTypeIDs)\n",
    "    })\n",
    "    must.M(err)\n",
    "    return embeddings\n",
    "}\n",
    "\n",
    "func p(modelID, target string) {\n",
    "    output := onnxTest(modelID, false, target)[0]\n",
    "    fmt.Printf(\"\\n%s:\\n\", target)\n",
    "    fmt.Printf(\"%s\\n\", output)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e62783-71a4-4725-9b19-3ff8c33d5d94",
   "metadata": {},
   "source": [
    "### Last layer result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4de6e20-c0ae-47df-b9f0-6f6f86f6c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1/1 files, 266 MB downloaded         \n",
      "\n",
      "onnxport/distilbert-base-uncased-onnx\n",
      "ONNX Model:\n",
      "\tProducer:\tpytorch / 1.12.0\n",
      "\t# inputs:\t2\n",
      "\t\t[#0] \"input_ids\": (Int64) [batch, sequence]\n",
      "\t\t[#1] \"attention_mask\": (Int64) [batch, sequence]\n",
      "\t# outputs:\t1\n",
      "\t\t[#0] \"last_hidden_state\": (Float32) [batch, sequence, 768]\n",
      "\t# nodes:\t593\n",
      "\t# tensors (variables):\t106\n",
      "\t# sparse tensors (variables):\t0\n",
      "\tOp types:\t[]string{\"Add\", \"Cast\", \"Concat\", \"Constant\", \"Div\", \"Equal\", \"Erf\", \"Expand\", \"Gather\", \"Identity\", \"MatMul\", \"Mul\", \"Pow\", \"ReduceMean\", \"Reshape\", \"Shape\", \"Slice\", \"Softmax\", \"Sqrt\", \"Sub\", \"Transpose\", \"Unsqueeze\", \"Where\"}\n",
      "\tIR Version:\t6\n",
      "\tOperator Sets:\t[v11]\n",
      "\n",
      "\n",
      "\n",
      "Example invocation:\n",
      "\tsentences: [\"This is an example sentence\" \"Each sentence is converted\"]\n",
      "\touput #0: [2][7][768]float32{\n",
      " {{-0.2348, -0.2102, -0.0227, ..., -0.0947, 0.0382, 0.4671},\n",
      "  {-0.7355, -0.5579, -0.3003, ..., -0.3094, 0.3252, 0.4205},\n",
      "  {-0.6125, -0.3603, -0.0217, ..., -0.1959, -0.0726, 1.0924},\n",
      "  ...,\n",
      "  {-0.5531, 0.0468, -0.3205, ..., -0.4445, -0.1745, 0.2172},\n",
      "  {-0.1437, -0.0652, -0.2817, ..., -0.1090, -0.2701, -0.0627},\n",
      "  {0.9936, 0.0086, -0.4680, ..., 0.2159, -0.8766, -0.2008}},\n",
      " {{-0.2836, -0.0795, -0.0073, ..., 0.0229, 0.1350, 0.4443},\n",
      "  {-0.8433, 0.0516, 0.4854, ..., 0.0233, 0.2920, 0.2891},\n",
      "  {0.0494, -0.1104, -0.0149, ..., -0.0619, -0.1338, -0.4708},\n",
      "  ...,\n",
      "  {-0.0937, -0.2482, 0.0248, ..., 0.0607, 0.0022, 0.0099},\n",
      "  {0.9928, 0.1657, -0.3293, ..., 0.2002, -0.8282, -0.1864},\n",
      "  {-0.0085, -0.2937, 0.2316, ..., 0.0004, -0.0332, 0.4620}}}\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "fmt.Printf(\"\\touput #0: %s\\n\", onnxTest(distillbertID, true)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cbede-7ace-4eb2-b2f8-f3995e7c1f8c",
   "metadata": {},
   "source": [
    "### Parse ONNX model for several repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976b10d2-2793-4aef-830d-d21bddb22603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence-transformers/all-MiniLM-L6-v2:\n",
      "\tInputs: [\"input_ids\" \"attention_mask\" \"token_type_ids\"]\n",
      "\tOutput #0: last_hidden_state - (Float32) [batch_size, sequence_length, 384]\n",
      "\tEmbeddings:\t[2][7][384]float32{\n",
      " {{0.0366, -0.0162, 0.1682, ..., 0.0554, -0.1644, -0.2967},\n",
      "  {0.7239, 0.6399, 0.1888, ..., 0.5946, 0.6206, 0.4897},\n",
      "  {0.0064, 0.0203, 0.0448, ..., 0.3464, 1.3170, -0.1670},\n",
      "  ...,\n",
      "  {0.1479, -0.0643, 0.1457, ..., 0.8837, -0.3316, 0.2975},\n",
      "  {0.5212, 0.6563, 0.5607, ..., -0.0399, 0.0412, -1.4036},\n",
      "  {1.0824, 0.7140, 0.3986, ..., -0.2301, 0.3243, -1.0313}},\n",
      " {{0.2802, 0.1165, -0.0418, ..., 0.2711, -0.1685, -0.2961},\n",
      "  {0.8729, 0.4545, -0.1091, ..., 0.1365, 0.4580, -0.2042},\n",
      "  {0.4752, 0.5731, 0.6304, ..., 0.6526, 0.5612, -1.3268},\n",
      "  ...,\n",
      "  {0.6113, 0.7920, -0.4685, ..., 0.0854, 1.0592, -0.2983},\n",
      "  {0.4115, 1.0946, 0.2385, ..., 0.8984, 0.3684, -0.7333},\n",
      "  {0.1374, 0.5555, 0.2678, ..., 0.5426, 0.4665, -0.5284}}}\n",
      "\n",
      "protectai/deberta-v3-base-zeroshot-v1-onnx:\n",
      "\tInputs: [\"input_ids\" \"attention_mask\"]\n",
      "\tOutput #0: logits - (Float32) [batch_size, 2]\n",
      "\tEmbeddings:\t[2][2]float32{\n",
      " {-3.5751, 3.4023}},\n",
      " {-2.9113, 2.9394}}\n",
      "\n",
      "KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english:\n",
      "\tInputs: [\"input_ids\" \"attention_mask\"]\n",
      "\tOutput #0: logits - (Float32) [batch_size, 2]\n",
      "\tEmbeddings:\t[2][2]float32{\n",
      " {-1.8679, 1.7647}},\n",
      " {1.4955, -1.3462}}\n",
      "\n",
      "KnightsAnalytics/distilbert-NER:\n",
      "\tInputs: [\"input_ids\" \"attention_mask\"]\n",
      "\tOutput #0: logits - (Float32) [batch_size, sequence_length, 9]\n",
      "\tEmbeddings:\t[2][7][9]float32{\n",
      " {{7.0331, -0.1794, -1.5561, ..., -1.3033, -0.4322, -1.5858},\n",
      "  {8.2202, -0.4533, -1.9596, ..., -2.0179, -0.4971, -1.8319},\n",
      "  {9.3073, -0.4040, -1.7569, ..., -1.7475, -1.4538, -1.5460},\n",
      "  ...,\n",
      "  {9.4321, -0.6450, -1.7683, ..., -1.6711, -0.9610, -1.6652},\n",
      "  {9.0240, -0.6443, -1.4341, ..., -1.3899, -1.6808, -1.2467},\n",
      "  {6.7673, 0.3862, -1.6108, ..., -1.2095, -0.3476, -0.5290}},\n",
      " {{7.0376, -1.0962, -2.4071, ..., -1.6919, 0.8237, -1.2293},\n",
      "  {1.0937, -1.3129, -1.7949, ..., -2.5516, 7.2302, -0.2294},\n",
      "  {8.5414, -1.7554, -1.9821, ..., -1.8467, 0.0536, -0.5844},\n",
      "  ...,\n",
      "  {8.7302, -0.7292, -1.7514, ..., -1.7240, -1.7068, -0.7294},\n",
      "  {5.5813, -0.1505, -1.5533, ..., -1.2480, 1.1509, 0.5053},\n",
      "  {5.0303, 1.4254, -2.7975, ..., -3.3346, 1.7287, -1.5107}}}\n",
      "\n",
      "SamLowe/roberta-base-go_emotions-onnx:\n",
      "\tInputs: [\"input_ids\" \"attention_mask\"]\n",
      "\tOutput #0: logits - (Float32) [batch_size, 28]\n",
      "\tEmbeddings:\t[2][28]float32{\n",
      " {-5.6430, -6.0157, -5.8392, ..., -5.7518, -6.8059, 3.3100}},\n",
      " {-5.7150, -5.9993, -5.7604, ..., -5.7850, -6.3915, 3.4068}}\n"
     ]
    }
   ],
   "source": [
    "import (\n",
    "\t\"github.com/gomlx/go-huggingface/hub\"\n",
    ")\n",
    "\n",
    "var (\n",
    "    modelIDs = []string {\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"protectai/deberta-v3-base-zeroshot-v1-onnx\",\n",
    "        \"KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"KnightsAnalytics/distilbert-NER\",\n",
    "        \"SamLowe/roberta-base-go_emotions-onnx\",\n",
    "    }\n",
    "    hfAuthToken = os.Getenv(\"HF_TOKEN\")\n",
    "\n",
    "    sentences = []string{\n",
    "        \"This is an example sentence\", \n",
    "        \"Each sentence is converted\"}\n",
    ")\n",
    "\n",
    "%%\n",
    "for _, modelID := range modelIDs {\n",
    "    fmt.Printf(\"\\n%s:\\n\", modelID)\n",
    "    repo := hub.New(modelID).WithAuth(hfAuthToken)\n",
    "    must.M(repo.DownloadInfo(false))  // Not needed, but reports an error if repo info could not be downloaded.\n",
    "    modelFileName := \"model.onnx\"\n",
    "    if !repo.HasFile(modelFileName) { \n",
    "        modelFileName = \"onnx/model.onnx\"\n",
    "        if !repo.HasFile(modelFileName) { \n",
    "            log.Printf(\"Could not find \\\"model.onnx\\\" for repo %q\", repo)\n",
    "        }\n",
    "    }\n",
    "    modelPath := must.M1(repo.DownloadFile(modelFileName))\n",
    "    model := must.M1(onnx.ReadFile(modelPath))\n",
    "    inputNames, _ := model.Inputs()\n",
    "    fmt.Printf(\"\\tInputs: %q\\n\", inputNames)\n",
    "    outputNames, outputShapes := model.Outputs()\n",
    "    for ii, n := range outputNames {\n",
    "        fmt.Printf(\"\\tOutput #%d: %s - %s\\n\", ii, n, outputShapes[ii])\n",
    "    }\n",
    "\n",
    "    // Convert ONNX variables to GoMLX context (which stores variables):\n",
    "    ctx := context.New()\n",
    "    must.M(model.VariablesToContext(ctx))\n",
    "\n",
    "    // TODO: use github.com/gomlx/go-huggingface to tokenize according to the model.\n",
    "    inputIDs := [][]int64{\n",
    "        {101, 2023, 2003, 2019, 2742, 6251,  102},\n",
    "        { 101, 2169, 6251, 2003, 4991,  102,    0}}\n",
    "    tokenTypeIDs := [][]int64{\n",
    "        {0, 0, 0, 0, 0, 0, 0},\n",
    "        {0, 0, 0, 0, 0, 0, 0}}\n",
    "    attentionMask := [][]int64{\n",
    "        {1, 1, 1, 1, 1, 1, 1},\n",
    "        {1, 1, 1, 1, 1, 1, 0}}\n",
    "\n",
    "    \n",
    "    embeddings := context.ExecOnce(\n",
    "        backend, ctx, \n",
    "        func (ctx *context.Context, inputs []*Node) *Node {\n",
    "            inputsMap := map[string]*Node{\n",
    "                \"input_ids\": inputs[0],\n",
    "                \"attention_mask\": inputs[1]}\n",
    "            if len(inputNames) == 3 {\n",
    "                inputsMap[\"token_type_ids\"] = inputs[2]\n",
    "            }\n",
    "            modelOutputs := model.CallGraph(ctx, inputs[0].Graph(), inputsMap)\n",
    "            return modelOutputs[0]\n",
    "        }, inputIDs, attentionMask, tokenTypeIDs)    \n",
    "    fmt.Printf(\"\\tEmbeddings:\\t%s\\n\", embeddings)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (gonb)",
   "language": "go",
   "name": "gonb"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".go",
   "mimetype": "",
   "name": "go",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "go1.23.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
