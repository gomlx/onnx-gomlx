{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe7380b-e9b9-457a-bdf3-eabeae4b43d3",
   "metadata": {},
   "source": [
    "# Create Linear Model trivial ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "39897ce6-4ab9-44b8-8262-aad96635d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import TensorProto\n",
    "from onnx.helper import (\n",
    "    make_model, make_node, make_graph,\n",
    "    make_tensor_value_info)\n",
    "from onnx.checker import check_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a5111e-9542-4a90-b33e-7809b092a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 5\n",
    "X = make_tensor_value_info('X', TensorProto.FLOAT, [\"batch_size\", feature_dim])\n",
    "Y = make_tensor_value_info('Y', TensorProto.FLOAT, [\"batch_size\"])\n",
    "A_initializer = onnx.helper.make_tensor('A', TensorProto.FLOAT, [feature_dim], [100.0, 10.0, 1.0, 0.1, 0.01])\n",
    "B_initializer = onnx.helper.make_tensor('B', TensorProto.FLOAT, [], [7000.0])\n",
    "node1 = make_node('MatMul', ['X', 'A'], ['XA'], 'XA')\n",
    "node2 = make_node('Add', ['XA', 'B'], ['Y'], 'Y')\n",
    "graph = make_graph([node1, node2], 'lr', [X], [Y], initializer=[A_initializer, B_initializer])\n",
    "onnx_model = make_model(graph)\n",
    "check_model(onnx_model)\n",
    "with open(\"linear_regression.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740e9376-6c3b-4c0f-b15c-2e7c10f80174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]]\n",
      "[7123.45 7679.  ]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(10, dtype=np.float32)+1\n",
    "x = np.reshape(x, (2, 5))\n",
    "print(x)\n",
    "ort_sess = ort.InferenceSession('linear_regression.onnx')\n",
    "outputs = ort_sess.run(['Y'], {'X': x})\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2477f0c-1ed3-4c39-aa1c-2a76dfb195c6",
   "metadata": {},
   "source": [
    "### MatMul WAT:  How does it work on the edge cases ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0933b3-8e72-4993-9029-35a329c20147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 7, 32)\n",
      "(12, 32, 7)\n",
      "(2, 12, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "lhs = (np.arange(2 * 1 * 7 * 32, dtype=np.float32)+1) / 1000.0\n",
    "lhs = np.reshape(lhs, (2, 1, 7, 32))\n",
    "print(lhs.shape)\n",
    "rhs = (np.arange(12*7*32, dtype=np.float32)+1) / 1000.0\n",
    "rhs = np.reshape(rhs, (12, 32, 7))\n",
    "print(rhs.shape)\n",
    "res = np.matmul(rhs, lhs)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a6f50-c45a-47cb-a297-78b92b525cf9",
   "metadata": {},
   "source": [
    "# Experimenting with model [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "Normalization formulation:\n",
    "\n",
    "$$\n",
    "v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n",
    "$$\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c9d79b-48fa-41c8-929f-9850b587a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0e2fe-16c9-41cc-a4ec-fb6aaf4e28f8",
   "metadata": {},
   "source": [
    "### Imports, create `tokenizer` and `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b980b1-3745-40f6-98da-f22fe864c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 07:39:20.623591: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-31 07:39:20.752880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 07:39:20.795165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 07:39:20.811348: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 07:39:20.914578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 07:39:21.558344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b6904-6073-4070-bcbd-5e44a9ef799c",
   "metadata": {},
   "source": [
    "### Sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f7d2828-33ff-4e49-905e-d01214cfd1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input:\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "print(\"Encoded input:\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158d375-c48f-432c-a9c5-4324ebb24409",
   "metadata": {},
   "source": [
    "### Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aad9c06-6b29-43a9-b368-dd4af93a8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 384)\n",
      "[[[ 0.03656479 -0.01616146  0.1682453  ...  0.05540764 -0.16443957\n",
      "   -0.29669833]\n",
      "  [ 0.7239094   0.6399461   0.18878399 ...  0.5945502   0.6205655\n",
      "    0.489683  ]\n",
      "  [ 0.00637847  0.02030473  0.04475658 ...  0.34638238  1.3169885\n",
      "   -0.16695468]\n",
      "  ...\n",
      "  [ 0.1479177  -0.06426162  0.14569402 ...  0.8837387  -0.33155778\n",
      "    0.2975315 ]\n",
      "  [ 0.52124625  0.6562965   0.5607001  ... -0.03988977  0.04121367\n",
      "   -1.4035654 ]\n",
      "  [ 1.0824106   0.7140344   0.39859214 ... -0.23005268  0.32431406\n",
      "   -1.0312778 ]]\n",
      "\n",
      " [[ 0.2802185   0.11647302 -0.04178832 ...  0.27105364 -0.16846775\n",
      "   -0.29611403]\n",
      "  [ 0.87294626  0.4544794  -0.10909736 ...  0.13654931  0.45797268\n",
      "   -0.20415133]\n",
      "  [ 0.4751616   0.5731077   0.63044137 ...  0.6525696   0.5612419\n",
      "   -1.3268433 ]\n",
      "  ...\n",
      "  [ 0.61133045  0.79203445 -0.4684846  ...  0.08543227  1.0591549\n",
      "   -0.2983293 ]\n",
      "  [ 0.4115055   1.0945691   0.23854384 ...  0.8983636   0.3683571\n",
      "   -0.733289  ]\n",
      "  [ 0.13744976  0.55554354  0.26777348 ...  0.5426259   0.46651605\n",
      "   -0.52835524]]]\n"
     ]
    }
   ],
   "source": [
    "ort_sess = ort.InferenceSession('model.onnx')\n",
    "outputKey = 'last_hidden_state'\n",
    "inputs = {key: value.numpy() for key, value in encoded_input.data.items()}\n",
    "modelOutput = ort_sess.run([outputKey], inputs)[0]\n",
    "print(f\"{modelOutput.shape}\")\n",
    "print(modelOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a41d2981-06a6-452d-a703-2947dff64346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5763b114-bb6d-4ea0-af92-e65ea136d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probeOnnxNodeOutput(node_output_name, inputs):\n",
    "    model = onnx.load(\"model.onnx\")\n",
    "    del model.graph.output[:]\n",
    "    model.graph.output.append(onnx.ValueInfoProto(name=node_output_name))\n",
    "    assert len(model.graph.output) == 1\n",
    "    onnx.save(model, \"modified_model.onnx\")    \n",
    "    ort_sess = ort.InferenceSession('modified_model.onnx')\n",
    "    return ort_sess.run([node_output_name], inputs)[0]\n",
    "\n",
    "def p(node_output_name):\n",
    "    output = probeOnnxNodeOutput(node_output_name, inputs)\n",
    "    print(f\"\\n{node_output_name}: f{output.shape}\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "29bd8837-7704-4607-9fb1-1376c49927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_shapes.txt', 'a') as f:\n",
    "    for node in model.graph.node:\n",
    "        for node_output_name in node.output:\n",
    "            output = probeOnnxNodeOutput(node_output_name, inputs)\n",
    "            print(f\"{node_output_name}\\t{output.dtype}\\t{output.shape}\", file=f)\n",
    "            f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "943ad75a-4d03-4047-9af5-acc696c1f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/embeddings/Add_1_output_0: f(2, 7, 384)\n",
      "[[[-0.08855709 -0.03675481  0.01803644 ...  0.02607179  0.09117168\n",
      "   -0.01518174]\n",
      "  [-0.02002142 -0.00136943 -0.01765827 ...  0.02036703  0.05219622\n",
      "    0.19905484]\n",
      "  [-0.01959006 -0.03363657 -0.03186595 ...  0.02031087  0.07087033\n",
      "    0.06444595]\n",
      "  ...\n",
      "  [-0.02530987  0.04081389  0.01253615 ... -0.02695212  0.03774461\n",
      "    0.11325061]\n",
      "  [-0.01395568 -0.02749825  0.07956143 ... -0.07483339  0.07742585\n",
      "   -0.06570429]\n",
      "  [ 0.03182676 -0.00320992 -0.02103326 ...  0.03869266  0.01906986\n",
      "   -0.00592621]]\n",
      "\n",
      " [[-0.08855709 -0.03675481  0.01803644 ...  0.02607179  0.09117168\n",
      "   -0.01518174]\n",
      "  [ 0.03040212  0.05308453 -0.02380589 ... -0.10111795  0.02182422\n",
      "    0.0473295 ]\n",
      "  [-0.00270701 -0.05080456  0.08054851 ... -0.07771945  0.08808091\n",
      "   -0.05600649]\n",
      "  ...\n",
      "  [ 0.0927911   0.01653565 -0.09761265 ...  0.04492704  0.03896102\n",
      "   -0.01817189]\n",
      "  [ 0.02310666  0.00902908 -0.02130682 ...  0.02319211  0.01912827\n",
      "   -0.00660186]\n",
      "  [-0.02132826  0.00192266  0.00427087 ...  0.05611002  0.01698602\n",
      "    0.02561522]]]\n"
     ]
    }
   ],
   "source": [
    "# p(\"/embeddings/Slice_output_0\")\n",
    "# p(\"/embeddings/position_embeddings/Gather_output_0\")\n",
    "#p(\"token_type_ids\")\n",
    "#p(\"embeddings.token_type_embeddings.weight\")\n",
    "#p(\"/embeddings/token_type_embeddings/Gather_output_0\")\n",
    "# p(\"/embeddings/Add_output_0\")\n",
    "p(\"/embeddings/Add_1_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ca9f7-cef6-41a6-983b-d7b01ab28db4",
   "metadata": {},
   "source": [
    "### Model Inference with HuggingFace/PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99cc66c8-139b-4b4f-9de5-49a38de3a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 384])\n",
      "tensor([[[ 0.0366, -0.0162,  0.1682,  ...,  0.0554, -0.1644, -0.2967],\n",
      "         [ 0.7239,  0.6399,  0.1888,  ...,  0.5946,  0.6206,  0.4897],\n",
      "         [ 0.0064,  0.0203,  0.0448,  ...,  0.3464,  1.3170, -0.1670],\n",
      "         ...,\n",
      "         [ 0.1479, -0.0643,  0.1457,  ...,  0.8837, -0.3316,  0.2975],\n",
      "         [ 0.5212,  0.6563,  0.5607,  ..., -0.0399,  0.0412, -1.4036],\n",
      "         [ 1.0824,  0.7140,  0.3986,  ..., -0.2301,  0.3243, -1.0313]],\n",
      "\n",
      "        [[ 0.2802,  0.1165, -0.0418,  ...,  0.2711, -0.1685, -0.2961],\n",
      "         [ 0.8729,  0.4545, -0.1091,  ...,  0.1365,  0.4580, -0.2042],\n",
      "         [ 0.4752,  0.5731,  0.6304,  ...,  0.6526,  0.5612, -1.3268],\n",
      "         ...,\n",
      "         [ 0.6113,  0.7920, -0.4685,  ...,  0.0854,  1.0592, -0.2983],\n",
      "         [ 0.4115,  1.0946,  0.2385,  ...,  0.8984,  0.3684, -0.7333],\n",
      "         [ 0.1374,  0.5555,  0.2678,  ...,  0.5426,  0.4665, -0.5284]]])\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "print(f\"{model_output.last_hidden_state.shape}\")\n",
    "print(model_output.last_hidden_state)\n",
    "\n",
    "if False:\n",
    "    # Disabled for now\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    print(f\"Sentence embeddings: {sentence_embeddings.shape}\")\n",
    "    print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfb072-1929-4a65-a1e1-aff4c7a1fa9f",
   "metadata": {},
   "source": [
    "## LSTM model example\n",
    "\n",
    "This will build a trivial PyTorch LSTM model, and save it (randomly initialized) to a ONNX model. Then we read the model and take note of the output with a fixed input, to compare with the GoMLX implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6ef71e-0612-47f5-96ef-914b451e9826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0c6253-3962-4734-80f0-8b7e1c03dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6]], dtype=torch.int32)\n",
      "tensor([[0.1168, 0.1587, 0.1992]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        hn = hn.squeeze(0)\n",
    "        out = self.fc(hn)\n",
    "        return out\n",
    "\n",
    "model = TextClassificationModel(30522, 5, 11, 3)\n",
    "test_input = torch.tensor([[0, 1, 2, 3, 4, 5, 6]], dtype=torch.int32)\n",
    "print(test_input)\n",
    "print(model(test_input))\n",
    "\n",
    "onnx_file_path = \"test_lstm.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    test_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {1: \"sequence_length\"}},\n",
    "    opset_version=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "936051e3-b91a-45a9-a4f1-ec78ce1a4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input =  \t[[0 1 2 3 4 5 6]]\n",
      "lstm(x) =\t[[0.11684047 0.15874878 0.19921872]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(7, dtype=np.int32)\n",
    "x = np.reshape(x, (1, 7))\n",
    "print(f\"input =  \\t{x}\")\n",
    "ort_sess = ort.InferenceSession('test_lstm.onnx')\n",
    "outputs = ort_sess.run(['output'], {'input': x})\n",
    "print(f\"lstm(x) =\\t{outputs[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
